{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import re\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the whole embedding into memory\n",
    "# embeddings_index = dict()\n",
    "# f = open('/media/hieu/CA48F77D48F7669B/WordVectorPretrained/model.txt', encoding = \"ISO-8859-1\")\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     if len(values) == 101:\n",
    "#         word = values[0]\n",
    "#         coefs = asarray(values[1:], dtype='float32')\n",
    "#     elif len(values) == 102:\n",
    "#         word = values[0] + values[1]\n",
    "#         coefs = asarray(values[2:], dtype='float32')\n",
    "#     try:\n",
    "#         embeddings_index[word] = coefs\n",
    "#     except:\n",
    "#         print(word)\n",
    "# f.close()\n",
    "# print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "    for i,word in enumerate(set(tokens)):\n",
    "        word_to_id[word] = i\n",
    "        id_to_word[i] = word\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file .........\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "entries = os.listdir(\"./mysmalltest\")\n",
    "#print(entries)\n",
    "data = \"\"\n",
    "for entry in entries:\n",
    "    path = \"./mysmalltest/\" + entry\n",
    "    with open(path) as f:\n",
    "        currentdata = f.read()\n",
    "    data = data + currentdata\n",
    "print(\"Loading data from file .........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token length 62487\n",
      "vocab length 4881\n"
     ]
    }
   ],
   "source": [
    "tokenszero = [\"\"]\n",
    "tokens = tokenize(data)\n",
    "tokens = tokenszero + tokens\n",
    "vocab = set(tokens)\n",
    "print(\"token length\",len(tokens))\n",
    "print(\"vocab length\",len(vocab))\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "#print(word_to_id)\n",
    "#print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sentence, word_to_id, window_size):\n",
    "    L = len(sentence)\n",
    "    X, Y = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for i in range(L):\n",
    "        index_before_target = list(range(max(0, i - window_size), i))           \n",
    "        index_after_target = list(range(i + 1, min(i + window_size + 1,L)))           \n",
    "        index_before_after_target = index_before_target + index_after_target\n",
    "        #print(index_before_after_target)                     \n",
    "        for j in index_before_after_target:\n",
    "            tempX.append(word_to_id[sentence[j]])\n",
    "        #print(tempX)\n",
    "        filling_missing_left = len(index_before_target)\n",
    "        filling_missing_right = len(index_after_target)\n",
    "        while(filling_missing_left < 3):\n",
    "            tempX.insert(0,0)\n",
    "            filling_missing_left +=1\n",
    "        while(filling_missing_right < 3):\n",
    "            tempX.append(0)\n",
    "            filling_missing_right +=1\n",
    "        X.append(tempX)\n",
    "        Y.append(word_to_id[sentence[i]])\n",
    "        tempX = []\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62487\n",
      "62487\n"
     ]
    }
   ],
   "source": [
    "X,Y = generate_training_data(tokens, word_to_id, 3)\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62487, 6)\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 6\n",
    "\n",
    "X = pad_sequences(X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62487, 4881)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "Y = to_categorical(Y, num_classes=vocab_size)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hieu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 100)            488100    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 150)               112950    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4881)              737031    \n",
      "=================================================================\n",
      "Total params: 1,338,081\n",
      "Trainable params: 1,338,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=6))\n",
    "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hieu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " - 25s - loss: 6.9431 - acc: 0.0220\n",
      "Epoch 2/100\n",
      " - 25s - loss: 6.4565 - acc: 0.0403\n",
      "Epoch 3/100\n",
      " - 24s - loss: 5.8162 - acc: 0.0888\n",
      "Epoch 4/100\n",
      " - 25s - loss: 4.9968 - acc: 0.1791\n",
      "Epoch 5/100\n",
      " - 25s - loss: 4.2938 - acc: 0.2609\n",
      "Epoch 6/100\n",
      " - 23s - loss: 3.7901 - acc: 0.3154\n",
      "Epoch 7/100\n",
      " - 23s - loss: 3.4042 - acc: 0.3616\n",
      "Epoch 8/100\n",
      " - 23s - loss: 3.0799 - acc: 0.4062\n",
      "Epoch 9/100\n",
      " - 23s - loss: 2.8137 - acc: 0.4450\n",
      "Epoch 10/100\n",
      " - 24s - loss: 2.5914 - acc: 0.4790\n",
      "Epoch 11/100\n",
      " - 23s - loss: 2.3967 - acc: 0.5078\n",
      "Epoch 12/100\n",
      " - 23s - loss: 2.2424 - acc: 0.5312\n",
      "Epoch 13/100\n",
      " - 23s - loss: 2.1014 - acc: 0.5523\n",
      "Epoch 14/100\n",
      " - 24s - loss: 1.9815 - acc: 0.5738\n",
      "Epoch 15/100\n",
      " - 24s - loss: 1.8817 - acc: 0.5866\n",
      "Epoch 16/100\n",
      " - 24s - loss: 1.7881 - acc: 0.6047\n",
      "Epoch 17/100\n",
      " - 24s - loss: 1.7106 - acc: 0.6152\n",
      "Epoch 18/100\n",
      " - 24s - loss: 1.6316 - acc: 0.6308\n",
      "Epoch 19/100\n",
      " - 24s - loss: 1.5674 - acc: 0.6399\n",
      "Epoch 20/100\n",
      " - 25s - loss: 1.5036 - acc: 0.6508\n",
      "Epoch 21/100\n",
      " - 25s - loss: 1.4465 - acc: 0.6615\n",
      "Epoch 22/100\n",
      " - 25s - loss: 1.3929 - acc: 0.6699\n",
      "Epoch 23/100\n",
      " - 25s - loss: 1.3470 - acc: 0.6781\n",
      "Epoch 24/100\n",
      " - 25s - loss: 1.3060 - acc: 0.6843\n",
      "Epoch 25/100\n",
      " - 25s - loss: 1.2737 - acc: 0.6905\n",
      "Epoch 26/100\n",
      " - 25s - loss: 1.2366 - acc: 0.6984\n",
      "Epoch 27/100\n",
      " - 25s - loss: 1.2024 - acc: 0.7046\n",
      "Epoch 28/100\n",
      " - 25s - loss: 1.1691 - acc: 0.7105\n",
      "Epoch 29/100\n",
      " - 25s - loss: 1.1421 - acc: 0.7152\n",
      "Epoch 30/100\n",
      " - 25s - loss: 1.1156 - acc: 0.7220\n",
      "Epoch 31/100\n",
      " - 25s - loss: 1.0824 - acc: 0.7287\n",
      "Epoch 32/100\n",
      " - 25s - loss: 1.0574 - acc: 0.7317\n",
      "Epoch 33/100\n",
      " - 25s - loss: 1.0393 - acc: 0.7362\n",
      "Epoch 34/100\n",
      " - 25s - loss: 1.0156 - acc: 0.7386\n",
      "Epoch 35/100\n",
      " - 25s - loss: 0.9969 - acc: 0.7427\n",
      "Epoch 36/100\n",
      " - 25s - loss: 0.9780 - acc: 0.7472\n",
      "Epoch 37/100\n",
      " - 25s - loss: 0.9594 - acc: 0.7527\n",
      "Epoch 38/100\n",
      " - 25s - loss: 0.9347 - acc: 0.7561\n",
      "Epoch 39/100\n",
      " - 25s - loss: 0.9311 - acc: 0.7570\n",
      "Epoch 40/100\n",
      " - 25s - loss: 0.9207 - acc: 0.7608\n",
      "Epoch 41/100\n",
      " - 25s - loss: 0.8955 - acc: 0.7649\n",
      "Epoch 42/100\n",
      " - 25s - loss: 0.8829 - acc: 0.7680\n",
      "Epoch 43/100\n",
      " - 26s - loss: 0.8695 - acc: 0.7690\n",
      "Epoch 44/100\n",
      " - 25s - loss: 0.8593 - acc: 0.7728\n",
      "Epoch 45/100\n",
      " - 25s - loss: 0.8516 - acc: 0.7736\n",
      "Epoch 46/100\n",
      " - 25s - loss: 0.8306 - acc: 0.7774\n",
      "Epoch 47/100\n",
      " - 25s - loss: 0.8272 - acc: 0.7789\n",
      "Epoch 48/100\n",
      " - 25s - loss: 0.8098 - acc: 0.7824\n",
      "Epoch 49/100\n",
      " - 25s - loss: 0.7971 - acc: 0.7842\n",
      "Epoch 50/100\n",
      " - 25s - loss: 0.7934 - acc: 0.7842\n",
      "Epoch 51/100\n",
      " - 25s - loss: 0.7822 - acc: 0.7844\n",
      "Epoch 52/100\n",
      " - 25s - loss: 0.7782 - acc: 0.7884\n",
      "Epoch 53/100\n",
      " - 25s - loss: 0.7745 - acc: 0.7889\n",
      "Epoch 54/100\n",
      " - 25s - loss: 0.7576 - acc: 0.7924\n",
      "Epoch 55/100\n",
      " - 25s - loss: 0.7548 - acc: 0.7932\n",
      "Epoch 56/100\n",
      " - 25s - loss: 0.7364 - acc: 0.7997\n",
      "Epoch 57/100\n",
      " - 25s - loss: 0.7428 - acc: 0.7946\n",
      "Epoch 58/100\n",
      " - 25s - loss: 0.7255 - acc: 0.7979\n",
      "Epoch 59/100\n",
      " - 25s - loss: 0.7159 - acc: 0.8019\n",
      "Epoch 60/100\n",
      " - 25s - loss: 0.7164 - acc: 0.8011\n",
      "Epoch 61/100\n",
      " - 25s - loss: 0.7046 - acc: 0.8057\n",
      "Epoch 62/100\n",
      " - 25s - loss: 0.7020 - acc: 0.8065\n",
      "Epoch 63/100\n",
      " - 25s - loss: 0.6913 - acc: 0.8072\n",
      "Epoch 64/100\n",
      " - 25s - loss: 0.6953 - acc: 0.8075\n",
      "Epoch 65/100\n",
      " - 25s - loss: 0.6908 - acc: 0.8089\n",
      "Epoch 66/100\n",
      " - 25s - loss: 0.6804 - acc: 0.8103\n",
      "Epoch 67/100\n",
      " - 25s - loss: 0.6748 - acc: 0.8120\n",
      "Epoch 68/100\n",
      " - 25s - loss: 0.6710 - acc: 0.8142\n",
      "Epoch 69/100\n",
      " - 25s - loss: 0.6727 - acc: 0.8119\n",
      "Epoch 70/100\n",
      " - 25s - loss: 0.6614 - acc: 0.8150\n",
      "Epoch 71/100\n",
      " - 25s - loss: 0.6551 - acc: 0.8158\n",
      "Epoch 72/100\n",
      " - 25s - loss: 0.6542 - acc: 0.8170\n",
      "Epoch 73/100\n",
      " - 25s - loss: 0.6400 - acc: 0.8198\n",
      "Epoch 74/100\n",
      " - 25s - loss: 0.6491 - acc: 0.8163\n",
      "Epoch 75/100\n",
      " - 25s - loss: 0.6420 - acc: 0.8182\n",
      "Epoch 76/100\n",
      " - 25s - loss: 0.6299 - acc: 0.8217\n",
      "Epoch 77/100\n",
      " - 25s - loss: 0.6236 - acc: 0.8229\n",
      "Epoch 78/100\n",
      " - 25s - loss: 0.6220 - acc: 0.8239\n",
      "Epoch 79/100\n",
      " - 25s - loss: 0.6212 - acc: 0.8236\n",
      "Epoch 80/100\n",
      " - 25s - loss: 0.6273 - acc: 0.8223\n",
      "Epoch 81/100\n",
      " - 25s - loss: 0.6174 - acc: 0.8254\n",
      "Epoch 82/100\n",
      " - 25s - loss: 0.6114 - acc: 0.8251\n",
      "Epoch 83/100\n",
      " - 25s - loss: 0.6008 - acc: 0.8291\n",
      "Epoch 84/100\n",
      " - 25s - loss: 0.6060 - acc: 0.8259\n",
      "Epoch 85/100\n",
      " - 25s - loss: 0.5988 - acc: 0.8294\n",
      "Epoch 86/100\n",
      " - 25s - loss: 0.5925 - acc: 0.8299\n",
      "Epoch 87/100\n",
      " - 25s - loss: 0.5921 - acc: 0.8306\n",
      "Epoch 88/100\n",
      " - 25s - loss: 0.5875 - acc: 0.8326\n",
      "Epoch 89/100\n",
      " - 25s - loss: 0.5871 - acc: 0.8309\n",
      "Epoch 90/100\n",
      " - 25s - loss: 0.5833 - acc: 0.8327\n",
      "Epoch 91/100\n",
      " - 25s - loss: 0.5812 - acc: 0.8333\n",
      "Epoch 92/100\n",
      " - 25s - loss: 0.5736 - acc: 0.8352\n",
      "Epoch 93/100\n",
      " - 25s - loss: 0.5737 - acc: 0.8376\n",
      "Epoch 94/100\n",
      " - 25s - loss: 0.5691 - acc: 0.8354\n",
      "Epoch 95/100\n",
      " - 25s - loss: 0.5695 - acc: 0.8373\n",
      "Epoch 96/100\n",
      " - 25s - loss: 0.5692 - acc: 0.8370\n",
      "Epoch 97/100\n",
      " - 25s - loss: 0.5552 - acc: 0.8412\n",
      "Epoch 98/100\n",
      " - 25s - loss: 0.5641 - acc: 0.8384\n",
      "Epoch 99/100\n",
      " - 25s - loss: 0.5652 - acc: 0.8386\n",
      "Epoch 100/100\n",
      " - 25s - loss: 0.5535 - acc: 0.8397\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=100, verbose=2)\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input_string):\n",
    "    return input_string.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checking(sentence):\n",
    "    encode_sentence = encode_input(sentence)\n",
    "    X, y = generate_training_data(encode_sentence, word_to_id, 3)\n",
    "    error_words = []\n",
    "    for i in range(len(X)):\n",
    "        word = encode_sentence[i]\n",
    "        word_index = word_to_id[word]\n",
    "        yhat = model.predict_proba(np.array([X[i]]))\n",
    "        if yhat[:,word_index] < 0.1:\n",
    "            error_words.append(encode_sentence[i])\n",
    "        print(\"input:\",np.array([X[i]]))\n",
    "        print(\"index:\",i)\n",
    "        print(\"probability of ---- \", encode_sentence[i], \"----given surrouding is:\" ,yhat[:,word_index])\n",
    "        print(\"__________________________________________________________________________\")\n",
    "    return error_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[   0    0    0  199 3767 2306]]\n",
      "index: 0\n",
      "probability of ----  chiến ----given surrouding is: [0.11843435]\n",
      "__________________________________________________________________________\n",
      "input: [[   0    0 1292 3767 2306    0]]\n",
      "index: 1\n",
      "probability of ----  tranh ----given surrouding is: [0.32584605]\n",
      "__________________________________________________________________________\n",
      "input: [[   0 1292  199 2306    0    0]]\n",
      "index: 2\n",
      "probability of ----  thế ----given surrouding is: [0.9970222]\n",
      "__________________________________________________________________________\n",
      "input: [[1292  199 3767    0    0    0]]\n",
      "index: 3\n",
      "probability of ----  giới ----given surrouding is: [0.86991745]\n",
      "__________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_checking(\"chiến tranh thế giới\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[   0    0    0 2038 3767 2306]]\n",
      "index: 0\n",
      "probability of ----  chiến ----given surrouding is: [1.3140418e-06]\n",
      "__________________________________________________________________________\n",
      "input: [[   0    0 1292 3767 2306    0]]\n",
      "index: 1\n",
      "probability of ----  tránh ----given surrouding is: [1.7283796e-07]\n",
      "__________________________________________________________________________\n",
      "input: [[   0 1292 2038 2306    0    0]]\n",
      "index: 2\n",
      "probability of ----  thế ----given surrouding is: [0.9638856]\n",
      "__________________________________________________________________________\n",
      "input: [[1292 2038 3767    0    0    0]]\n",
      "index: 3\n",
      "probability of ----  giới ----given surrouding is: [0.01197058]\n",
      "__________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chiến', 'tránh', 'giới']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_checking(\"chiến tránh thế giới\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
